{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as mp\n",
    "\n",
    "\n",
    "##Spacy function\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "\n",
    "## Train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## Performance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tendencias</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tecnologias</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pais</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mundo</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economia</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deportes</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cultura</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  count(*)\n",
       "0   tendencias      1000\n",
       "1  tecnologias      1000\n",
       "2         pais      1000\n",
       "3        mundo      1000\n",
       "4     economia      1000\n",
       "5     deportes      1000\n",
       "6      cultura      1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cnnchile_7000.csv')\n",
    "df = df.drop([\"country\",\"media_outlet\", \"url\",\"date\",\"title\"],1)\n",
    "\n",
    "q=\"\"\"SELECT category, count(*) FROM df GROUP BY category ORDER BY count(*) DESC;\"\"\"\n",
    "result=sqldf(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nX = df_train['text'].astype(str)\\nylabels = df_train['category'].astype(str)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.25, random_state=0)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PRUEBAS REALES\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "ylabels = df['category'].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "def feature_extraction(text):\n",
    "    \n",
    "    mytokens = nlp(text)\n",
    "\n",
    "    #Guardamos las palabras como características si corresponden a ciertas categorias gramaticaless\n",
    "    mytokens = [ word for word in mytokens if word.pos_ in [\"NOUN\", \"ADJ\", \"VERB\"] ]\n",
    "    \n",
    "    #Transformamos las palabras en minusculas\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logistica utilizando CountVectorizer vs TfidfVectorizer \n",
    "\n",
    "\n",
    "Al usar **countVectorizer**, utilizamos un enfoque `bag of word` donde cada palabra tiene la cantidad de veces que la palabra aparece en el texto (count)\n",
    "\n",
    "Al usar **TfidfVectorizer** cambiamos el enfoque **countVectorizer** a un enfoque `Term frequency (tf)` y `Inverse data frequency (idf)` por lo cual a la estructura anterior se remplaza el enfoque de conteo por la frecuencia que tenga esa palabra en el texto de la fila.\n",
    "\n",
    "Este enfoque es importnate debido a que disminuimos la importnancia de palabras comunes dentro de nuestro bag of words, asi por ejemplo palabras como *\"el, la, como\"* son menos propensas a afectar en el resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = feature_extraction, min_df=0., max_df=1.0)\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = feature_extraction, min_df=0., max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe1 = Pipeline([('vectorizing', bow_vector),\n",
    "                 ('learning', model_1)])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train,y_train)\n",
    "\n",
    "pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_1 = pipe1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_2 = pipe2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using CountVectorizer: 0.7222857142857143\n",
      "Logistic Regression using TfidfVectorizer: 0.772\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression using CountVectorizer:\",metrics.accuracy_score(y_test, predicted_model_1))\n",
    "\n",
    "print(\"Logistic Regression using TfidfVectorizer:\",metrics.accuracy_score(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para CountVectorizer: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.59      0.97      0.74       250\n",
      "    deportes       0.86      0.78      0.82       255\n",
      "    economia       0.74      0.82      0.78       273\n",
      "       mundo       0.72      0.71      0.72       238\n",
      "        pais       0.80      0.58      0.68       250\n",
      " tecnologias       0.69      0.72      0.70       252\n",
      "  tendencias       0.79      0.43      0.56       232\n",
      "\n",
      "    accuracy                           0.72      1750\n",
      "   macro avg       0.74      0.72      0.71      1750\n",
      "weighted avg       0.74      0.72      0.72      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para CountVectorizer: \")\n",
    "print(classification_report(y_test, predicted_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para TfidfVectorizer: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.87      0.90      0.89       250\n",
      "    deportes       0.85      0.87      0.86       255\n",
      "    economia       0.77      0.78      0.78       273\n",
      "       mundo       0.72      0.72      0.72       238\n",
      "        pais       0.78      0.68      0.72       250\n",
      " tecnologias       0.72      0.72      0.72       252\n",
      "  tendencias       0.69      0.72      0.70       232\n",
      "\n",
      "    accuracy                           0.77      1750\n",
      "   macro avg       0.77      0.77      0.77      1750\n",
      "weighted avg       0.77      0.77      0.77      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para TfidfVectorizer: \")\n",
    "print(classification_report(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios\n",
    "\n",
    "Podemos apreciar una clara diferencia entre la implementación de **LogisticRegression** utilizando `CountVectorizer` con la de `TfidfVectorizer`, para realizar esta comparación compararemos por separado los resultados de f1-score, con los de precision y recall, pero para ambos casos nos enfocaremos en las 2 clases las cuales tienen un cambio más significativo, **cultura**, **deportes**.\n",
    "\n",
    "* **f1-score:**  Podemos apreciar claramente que en todas las clases, Tfidf fue mejor, y en las 2 clases más problematicas pudo mejorar el rendimiento de manera considerable, donde en cultura, pasó de un score de 0.74 a 0.89 y en tendencias de 0.56 a 0.70. \n",
    "\n",
    "\n",
    "* **precision y recall:**  En la primera implementación, podemos observar una clara diferencia en la precision y recall de las clases **cultura** y **deportes**. En la clase cultura, podemos ver que existe un valor de recall sumamnete bueno, pero un modesto valor de precision, por lo que podemos inferir que el método está efecutando una mayor cantidad de falsos positivos que falsos negativos, y en la clase tendecias, podemos ver el caso contrario, debido a que el valor de precision es mayor muy bueno en comparación de recall. Este problema, es solucionado en la segunda implementación, pudiendo normalizar estos scores, haciendo asi una mejora en el valor de accuracy. \n",
    "\n",
    "Por lo que podemos concluir que la segunda implememtación es considerablemente mejor que la primera. \n",
    "Una razón de esta mejora, es que `TfidfVectorizer` le baja la importancia a las palabras comunes, por lo que es probable que en estas 2 clases, exista una gran cantidad de palabras comunes, haciendo que el modelo sea capaz de clasificar de mejor manera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tree.DecisionTreeClassifier()\n",
    "model_3 = clf = RandomForestClassifier(n_estimators=700,\n",
    "                                       max_depth=None, \n",
    "                                       random_state=0)\n",
    "\n",
    "pipe3 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_2)])\n",
    "\n",
    "pipe4 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning',\n",
       "                 RandomForestClassifier(n_estimators=700, random_state=0))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_2 = pipe3.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_3 = pipe4.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Decision Tree: 0.5422857142857143\n",
      "Accuracy Random Forest: 0.7325714285714285\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Decision Tree:\",metrics.accuracy_score(y_test, predicted_model_2))\n",
    "\n",
    "print(\"Accuracy Random Forest:\",metrics.accuracy_score(y_test, predicted_model_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para Decision Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.73      0.69      0.71       250\n",
      "    deportes       0.73      0.73      0.73       255\n",
      "    economia       0.57      0.54      0.56       273\n",
      "       mundo       0.39      0.44      0.41       238\n",
      "        pais       0.38      0.38      0.38       250\n",
      " tecnologias       0.56      0.53      0.54       252\n",
      "  tendencias       0.45      0.48      0.46       232\n",
      "\n",
      "    accuracy                           0.54      1750\n",
      "   macro avg       0.54      0.54      0.54      1750\n",
      "weighted avg       0.55      0.54      0.54      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para Decision Tree: \")\n",
    "print(classification_report(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para Random Forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.80      0.91      0.85       250\n",
      "    deportes       0.79      0.88      0.83       255\n",
      "    economia       0.74      0.74      0.74       273\n",
      "       mundo       0.73      0.63      0.68       238\n",
      "        pais       0.64      0.67      0.66       250\n",
      " tecnologias       0.73      0.66      0.70       252\n",
      "  tendencias       0.68      0.62      0.65       232\n",
      "\n",
      "    accuracy                           0.73      1750\n",
      "   macro avg       0.73      0.73      0.73      1750\n",
      "weighted avg       0.73      0.73      0.73      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para Random Forest: \")\n",
    "print(classification_report(y_test, predicted_model_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios\n",
    "\n",
    "A partir de los resultados podemos apreciar una clara mejora al implementar `RandomForestClassifier`, esto probablemente esto es debido a que algoritmo de `DecisionTree`, no es capaz de seleccionar bien las clases debido a la implementación **bag of words**. Una solución para esto, fue implementar `RandomForestClassifier` con un valor de **n_estimators** de 750, por lo que se crean 750 arboles de decision, los cuales son utilizados para definir a que clase partenece cada observación de *x_test*.\n",
    "\n",
    "Es probable que pueda existir un mejor valor de accuracy realizando un mejor **parameter tunning**, con los valores de Random Forest, debido a que solo se modificó el parametro anteriormente descrito, pero pese a esto, la accuracy de este algoritmo es muy cercano al de `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros modelos \n",
    "\n",
    "\n",
    "* Realizaremos pruebas de otros modelos de aprendizaje.Estos modelos son:\n",
    "> * SGDClassifier\n",
    "> * KNN (K-nearest Neighbor)\n",
    "\n",
    "Además a estos modelos se les aplicará un pequeño algoritmo para determinar los mejores parametros y por ende tener sus mejores valores de accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = SGDClassifier(loss='hinge', \n",
    "              penalty='l2', \n",
    "              alpha=1e-3, \n",
    "              random_state=None,\n",
    "              max_iter=1000, \n",
    "              tol=None)\n",
    "\n",
    "pipe5 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', SGDClassifier(alpha=0.001, tol=None))])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_4 = pipe5.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Accuracy: 0.7634285714285715\n"
     ]
    }
   ],
   "source": [
    "# Exactitud del modelo.\n",
    "print(\"SGDClassifier Accuracy:\",metrics.accuracy_score(y_test, predicted_model_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.79      0.95      0.86       250\n",
      "    deportes       0.84      0.92      0.88       255\n",
      "    economia       0.75      0.85      0.79       273\n",
      "       mundo       0.74      0.71      0.73       238\n",
      "        pais       0.77      0.63      0.69       250\n",
      " tecnologias       0.71      0.71      0.71       252\n",
      "  tendencias       0.73      0.55      0.63       232\n",
      "\n",
      "    accuracy                           0.76      1750\n",
      "   macro avg       0.76      0.76      0.76      1750\n",
      "weighted avg       0.76      0.76      0.76      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted_model_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar los mejores parametros usando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression best C parameter and run time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C parameter : 0.0001, accucary : 0.13257142857142856\n",
      " C parameter : 0.001, accucary : 0.20057142857142857\n",
      " C parameter : 0.01, accucary : 0.7182857142857143\n",
      " C parameter : 0.1, accucary : 0.76\n",
      " C parameter : 1.0, accucary : 0.772\n",
      " C parameter : 10, accucary : 0.772\n",
      " C parameter : 100, accucary : 0.7662857142857142\n",
      " C parameter : 1000, accucary : 0.7645714285714286\n",
      "Mejor accuracy usando el mejor C value:  0.772\n",
      "tiempo elegir mejor:1935 segundos\n"
     ]
    }
   ],
   "source": [
    "Cparameters = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 10, 100, 1000]\n",
    "\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "for i in range (len(Cparameters)):\n",
    "    model_1 = LogisticRegression(max_iter=1000, C=float(Cparameters[i]), n_jobs=-1)\n",
    "    \n",
    "    pipe2 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                      ('learning', model_1)])\n",
    "    \n",
    "    pipe2.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_1 = pipe2.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_1))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_1)\n",
    "    print(f\" C parameter : {str(Cparameters[i])}, accucary : {t}\")\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor C value: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDC best learning rate and run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learing rate : 0.00000, accucary : 0.7405714285714285\n",
      " learing rate : 0.00000, accucary : 0.7382857142857143\n",
      " learing rate : 0.00001, accucary : 0.7411428571428571\n",
      " learing rate : 0.00010, accucary : 0.7685714285714286\n",
      " learing rate : 0.00100, accucary : 0.76\n",
      " learing rate : 0.01000, accucary : 0.7474285714285714\n",
      " learing rate : 0.10000, accucary : 0.6491428571428571\n",
      " learing rate : 1.00000, accucary : 0.22685714285714287\n",
      "Mejor accuracy usando el mejor learning rate:  0.7685714285714286\n",
      "tiempo elegir mejor:1872 segundos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learningRateList = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0] # learning rate\n",
    "\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "for i in range (len(learningRateList)):\n",
    "    model_4 = SGDClassifier(loss='hinge', \n",
    "              penalty='l2', \n",
    "              alpha= float(learningRateList[i]), \n",
    "              random_state=None, n_jobs=-1)\n",
    "    \n",
    "    pipe5 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_4)])\n",
    "    \n",
    "    pipe5.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_4 = pipe5.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_4))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_4)\n",
    "    print(f\" learing rate : {float(learningRateList[i]):.5f}, accucary : {t}\")\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor learning rate: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Podemos observar en función de su valor de accuracy, que es posible parametrizar `SGDClassifier` de tal manera que obtenga un rendimiento muy similar a `LogisticRegression`, para poder llegar a este valor, se modificó el parametro **alpha** de `SGDClassifier`, el cual es un papametro regularizador y además define la taza de aprendizaje inicial, por lo que al definir valores de alpha más pequeños, es posible obtener mejores valores, pero por contraparterte es más lento a la hora de realizar el fit.\n",
    "\n",
    "Cabe destacar, que en la teoría, esperabamos obtener un tiempo de ejeción mucho menor en el algoritmo de `SGDClassifier` debido a que SGDClassifier es un clasificador lineal generalizado que utilizará Stochastic Gradient Descent y LogisticRegression no, si no que implementa una regresión logística logarítmica regularizada por lo que minimiza la probabilidad logarítmica. Pero este no fue el caso, ambos algoritmos tienen tiempos de ejecución similar, por lo que la decision de que modelo elegir no es clara, debido a que ambos algoritmos tienen un score muy similar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-nearest Neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N : 29, accucary : 0.7394285714285714\n",
      " N : 30, accucary : 0.7405714285714285\n",
      " N : 31, accucary : 0.7411428571428571\n",
      " N : 32, accucary : 0.7405714285714285\n",
      " N : 33, accucary : 0.74\n",
      " N : 34, accucary : 0.7388571428571429\n",
      " N : 35, accucary : 0.7411428571428571\n",
      " N : 36, accucary : 0.7394285714285714\n",
      " N : 37, accucary : 0.744\n",
      " N : 38, accucary : 0.7451428571428571\n",
      " N : 39, accucary : 0.7502857142857143\n",
      " N : 40, accucary : 0.7462857142857143\n",
      " N : 41, accucary : 0.7451428571428571\n",
      " N : 42, accucary : 0.7462857142857143\n",
      " N : 43, accucary : 0.7462857142857143\n",
      " N : 44, accucary : 0.7457142857142857\n",
      "Mejor accuracy usando el mejor vecino:  0.7502857142857143\n",
      "tiempo elegir mejor:3897 segundos\n"
     ]
    }
   ],
   "source": [
    "vecinos = range(29,45)\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "\n",
    "for vecino in vecinos:\n",
    "    model_5 = KNeighborsClassifier(vecino, n_jobs=4)\n",
    "\n",
    "    pipe6 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_5)])\n",
    "    pipe6.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_5 = pipe6.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_5))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_5)\n",
    "    print(f\" N : {int(vecino)}, accucary : {t}\")\n",
    "    \n",
    "end = time.time()\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor vecino: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Podemos observar que el accuracy de KNN es bastente cercano al mejor (LogisicRegresion), siendo este de 0.7502, utilizando 39 vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (7), usually from a call to set_ticks, does not match the number of ticklabels (19599).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-01d4fe5fca1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                  \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                  normalize=normalize)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[1;32m    231\u001b[0m     return disp.plot(include_values=include_values,\n\u001b[1;32m    232\u001b[0m                      \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks_rotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxticks_rotation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                      values_format=values_format)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, include_values, cmap, xticks_rotation, values_format, ax)\u001b[0m\n\u001b[1;32m    123\u001b[0m                \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"True label\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                xlabel=\"Predicted label\")\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmove_color_to_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfindobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    996\u001b[0m                         raise AttributeError(f\"{type(self).__name__!r} object \"\n\u001b[1;32m    997\u001b[0m                                              f\"has no property {k!r}\")\n\u001b[0;32m--> 998\u001b[0;31m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_set_ticklabels\u001b[0;34m(self, labels, fontdict, minor, **kwargs)\u001b[0m\n\u001b[1;32m   1791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfontdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_keyword_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"minor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[0;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1715\u001b[0;31m                     \u001b[0;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m                     \u001b[0;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m                     \u001b[0;34m\" set_ticks, does not match\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of FixedLocator locations (7), usually from a call to set_ticks, does not match the number of ticklabels (19599)."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(pipe1.fit(X_train, y_train), X_test, y_test,\n",
    "                                 display_labels=tfidf_vector.get_feature_names(),\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pytorch were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"pytorch\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"pytorch\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
