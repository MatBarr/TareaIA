{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as mp\n",
    "\n",
    "\n",
    "##Spacy function\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "\n",
    "## Train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## Performance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tendencias</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tecnologias</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pais</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mundo</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economia</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deportes</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cultura</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  count(*)\n",
       "0   tendencias      1000\n",
       "1  tecnologias      1000\n",
       "2         pais      1000\n",
       "3        mundo      1000\n",
       "4     economia      1000\n",
       "5     deportes      1000\n",
       "6      cultura      1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cnnchile_7000.csv')\n",
    "df = df.drop([\"country\",\"media_outlet\", \"url\",\"date\",\"title\"],1)\n",
    "\n",
    "q=\"\"\"SELECT category, count(*) FROM df GROUP BY category ORDER BY count(*) DESC;\"\"\"\n",
    "result=sqldf(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRUEBAS REALES\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "ylabels = df['category'].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "def feature_extraction(text):\n",
    "    \n",
    "    mytokens = nlp(text)\n",
    "\n",
    "    #Guardamos las palabras como características si corresponden a ciertas categorias gramaticaless\n",
    "    mytokens = [ word for word in mytokens if word.pos_ in [\"NOUN\", \"ADJ\", \"VERB\"] ]\n",
    "    \n",
    "    #Transformamos las palabras en minusculas\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logistica utilizando CountVectorizer vs TfidfVectorizer \n",
    "\n",
    "Al usar **countVectorizer**, utilizamos un enfoque `bag of word` donde cada palabra tiene la cantidad de veces que la palabra aparece en el texto (count)\n",
    "\n",
    "Al usar **TfidfVectorizer** cambiamos el enfoque **countVectorizer** a un enfoque `Term frequency (tf)` y `Inverse data frequency (idf)` por lo cual a la estructura anterior se reemplaza el enfoque de conteo por la frecuencia que tenga esa palabra en el texto de la fila.\n",
    "\n",
    "Este enfoque es importante debido a que disminuimos la importancia de palabras comunes dentro de nuestro bag of words, así por ejemplo palabras como *\"el, la, como\"* son menos propensas a afectar en el resultado final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = feature_extraction, min_df=0., max_df=1.0)\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = feature_extraction, min_df=0., max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe1 = Pipeline([('vectorizing', bow_vector),\n",
    "                 ('learning', model_1)])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train,y_train)\n",
    "\n",
    "pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_1 = pipe1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_2 = pipe2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using CountVectorizer: 0.7222857142857143\n",
      "Logistic Regression using TfidfVectorizer: 0.772\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression using CountVectorizer:\",metrics.accuracy_score(y_test, predicted_model_1))\n",
    "\n",
    "print(\"Logistic Regression using TfidfVectorizer:\",metrics.accuracy_score(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para CountVectorizer: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.59      0.97      0.74       250\n",
      "    deportes       0.86      0.78      0.82       255\n",
      "    economia       0.74      0.82      0.78       273\n",
      "       mundo       0.72      0.71      0.72       238\n",
      "        pais       0.80      0.58      0.68       250\n",
      " tecnologias       0.69      0.72      0.70       252\n",
      "  tendencias       0.79      0.43      0.56       232\n",
      "\n",
      "    accuracy                           0.72      1750\n",
      "   macro avg       0.74      0.72      0.71      1750\n",
      "weighted avg       0.74      0.72      0.72      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para CountVectorizer: \")\n",
    "print(classification_report(y_test, predicted_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para TfidfVectorizer: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.87      0.90      0.89       250\n",
      "    deportes       0.85      0.87      0.86       255\n",
      "    economia       0.77      0.78      0.78       273\n",
      "       mundo       0.72      0.72      0.72       238\n",
      "        pais       0.78      0.68      0.72       250\n",
      " tecnologias       0.72      0.72      0.72       252\n",
      "  tendencias       0.69      0.72      0.70       232\n",
      "\n",
      "    accuracy                           0.77      1750\n",
      "   macro avg       0.77      0.77      0.77      1750\n",
      "weighted avg       0.77      0.77      0.77      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para TfidfVectorizer: \")\n",
    "print(classification_report(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios\n",
    "\n",
    "Podemos apreciar una clara diferencia entre la implementación de **LogisticRegression** utilizando `CountVectorizer` con la de `TfidfVectorizer`, para realizar esta comparación compararemos por separado los resultados de f1-score, con los de precision y recall, pero para ambos casos nos enfocaremos en las 2 clases las cuales tienen un cambio más significativo, **cultura**, **deportes**.\n",
    "\n",
    "* **f1-score:**  Podemos apreciar claramente que en todas las clases, Tfidf fue mejor, y en las 2 clases más problemáticas pudo mejorar el rendimiento de manera considerable, donde en cultura, pasó de un score de 0.74 a 0.89 y en tendencias de 0.56 a 0.70.\n",
    "\n",
    "\n",
    "* **precision y recall:**  En la primera implementación, podemos observar una clara diferencia en la precision y recall de las clases **cultura** y **deportes**. En la clase cultura, podemos ver que existe un valor de recall sumamente bueno, pero un modesto valor de precision, por lo que podemos inferir que el método está efectuando una mayor cantidad de falsos positivos que falsos negativos, y en la clase tendencias, podemos ver el caso contrario, debido a que el valor de precision es mayor muy bueno en comparación de recall. Este problema, es solucionado en la segunda implementación, pudiendo normalizar estos scores, haciendo así una mejora en el valor de accuracy.\n",
    "\n",
    "Por lo que podemos concluir que la segunda implementación es considerablemente mejor que la primera.\n",
    "Una razón de esta mejora, es que `TfidfVectorizer` le baja la importancia a las palabras comunes, por lo que es probable que en estas 2 clases, exista una gran cantidad de palabras comunes, haciendo que el modelo sea capaz de clasificar de mejor manera.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tree.DecisionTreeClassifier()\n",
    "model_3 = clf = RandomForestClassifier(n_estimators=700,\n",
    "                                       max_depth=None, \n",
    "                                       random_state=0)\n",
    "\n",
    "pipe3 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_2)])\n",
    "\n",
    "pipe4 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning',\n",
       "                 RandomForestClassifier(n_estimators=700, random_state=0))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_2 = pipe3.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_3 = pipe4.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Decision Tree: 0.5422857142857143\n",
      "Accuracy Random Forest: 0.7325714285714285\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Decision Tree:\",metrics.accuracy_score(y_test, predicted_model_2))\n",
    "\n",
    "print(\"Accuracy Random Forest:\",metrics.accuracy_score(y_test, predicted_model_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para Decision Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.73      0.69      0.71       250\n",
      "    deportes       0.73      0.73      0.73       255\n",
      "    economia       0.57      0.54      0.56       273\n",
      "       mundo       0.39      0.44      0.41       238\n",
      "        pais       0.38      0.38      0.38       250\n",
      " tecnologias       0.56      0.53      0.54       252\n",
      "  tendencias       0.45      0.48      0.46       232\n",
      "\n",
      "    accuracy                           0.54      1750\n",
      "   macro avg       0.54      0.54      0.54      1750\n",
      "weighted avg       0.55      0.54      0.54      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para Decision Tree: \")\n",
    "print(classification_report(y_test, predicted_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión para Random Forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.80      0.91      0.85       250\n",
      "    deportes       0.79      0.88      0.83       255\n",
      "    economia       0.74      0.74      0.74       273\n",
      "       mundo       0.73      0.63      0.68       238\n",
      "        pais       0.64      0.67      0.66       250\n",
      " tecnologias       0.73      0.66      0.70       252\n",
      "  tendencias       0.68      0.62      0.65       232\n",
      "\n",
      "    accuracy                           0.73      1750\n",
      "   macro avg       0.73      0.73      0.73      1750\n",
      "weighted avg       0.73      0.73      0.73      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz de confusión para Random Forest: \")\n",
    "print(classification_report(y_test, predicted_model_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios\n",
    "\n",
    "A partir de los resultados podemos apreciar una clara mejora al implementar `RandomForestClassifier`, esto probablemente esto es debido a que algoritmo de `DecisionTree`, no es capaz de seleccionar bien las clases debido a la implementación **bag of words**. Una solución para esto, fue implementar `RandomForestClassifier` con un valor de **n_estimators** de 750, por lo que se crean 750 árboles de decisión, los cuales son utilizados para definir a qué clase pertenece cada observación de *x_test*.\n",
    "\n",
    "Es probable que pueda existir un mejor valor de accuracy realizando un mejor **parameter tuning**, con los valores de Random Forest, debido a que solo se modificó el parámetro anteriormente descrito, pero pese a esto, la accuracy de este algoritmo es muy cercano al de `LogisticRegression`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros modelos \n",
    "\n",
    "\n",
    "* Realizaremos pruebas de otros modelos de aprendizaje.Estos modelos son:\n",
    "> * SGDClassifier\n",
    "> * KNN (K-nearest Neighbor)\n",
    "\n",
    "Además a estos modelos se les aplicará un pequeño algoritmo para determinar los mejores parametros y por ende tener sus mejores valores de accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = SGDClassifier(loss='hinge', \n",
    "              penalty='l2', \n",
    "              alpha=1e-3, \n",
    "              random_state=None,\n",
    "              max_iter=1000, \n",
    "              tol=None)\n",
    "\n",
    "pipe5 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizing',\n",
       "                 TfidfVectorizer(min_df=0.0,\n",
       "                                 tokenizer=<function feature_extraction at 0x7f4ee703ef80>)),\n",
       "                ('learning', SGDClassifier(alpha=0.001, tol=None))])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_4 = pipe5.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Accuracy: 0.7634285714285715\n"
     ]
    }
   ],
   "source": [
    "# Exactitud del modelo.\n",
    "print(\"SGDClassifier Accuracy:\",metrics.accuracy_score(y_test, predicted_model_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cultura       0.79      0.95      0.86       250\n",
      "    deportes       0.84      0.92      0.88       255\n",
      "    economia       0.75      0.85      0.79       273\n",
      "       mundo       0.74      0.71      0.73       238\n",
      "        pais       0.77      0.63      0.69       250\n",
      " tecnologias       0.71      0.71      0.71       252\n",
      "  tendencias       0.73      0.55      0.63       232\n",
      "\n",
      "    accuracy                           0.76      1750\n",
      "   macro avg       0.76      0.76      0.76      1750\n",
      "weighted avg       0.76      0.76      0.76      1750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted_model_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar los mejores parametros usando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression best C parameter and run time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C parameter : 0.0001, accucary : 0.13257142857142856\n",
      " C parameter : 0.001, accucary : 0.20057142857142857\n",
      " C parameter : 0.01, accucary : 0.7182857142857143\n",
      " C parameter : 0.1, accucary : 0.76\n",
      " C parameter : 1.0, accucary : 0.772\n",
      " C parameter : 10, accucary : 0.772\n",
      " C parameter : 100, accucary : 0.7662857142857142\n",
      " C parameter : 1000, accucary : 0.7645714285714286\n",
      "Mejor accuracy usando el mejor C value:  0.772\n",
      "tiempo elegir mejor:1935 segundos\n"
     ]
    }
   ],
   "source": [
    "Cparameters = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 10, 100, 1000]\n",
    "\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "for i in range (len(Cparameters)):\n",
    "    model_1 = LogisticRegression(max_iter=1000, C=float(Cparameters[i]), n_jobs=-1)\n",
    "    \n",
    "    pipe2 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                      ('learning', model_1)])\n",
    "    \n",
    "    pipe2.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_1 = pipe2.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_1))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_1)\n",
    "    print(f\" C parameter : {str(Cparameters[i])}, accucary : {t}\")\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor C value: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDC best learning rate and run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learing rate : 0.00000, accucary : 0.7405714285714285\n",
      " learing rate : 0.00000, accucary : 0.7382857142857143\n",
      " learing rate : 0.00001, accucary : 0.7411428571428571\n",
      " learing rate : 0.00010, accucary : 0.7685714285714286\n",
      " learing rate : 0.00100, accucary : 0.76\n",
      " learing rate : 0.01000, accucary : 0.7474285714285714\n",
      " learing rate : 0.10000, accucary : 0.6491428571428571\n",
      " learing rate : 1.00000, accucary : 0.22685714285714287\n",
      "Mejor accuracy usando el mejor learning rate:  0.7685714285714286\n",
      "tiempo elegir mejor:1872 segundos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learningRateList = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0] # learning rate\n",
    "\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "for i in range (len(learningRateList)):\n",
    "    model_4 = SGDClassifier(loss='hinge', \n",
    "              penalty='l2', \n",
    "              alpha= float(learningRateList[i]), \n",
    "              random_state=None, n_jobs=-1)\n",
    "    \n",
    "    pipe5 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_4)])\n",
    "    \n",
    "    pipe5.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_4 = pipe5.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_4))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_4)\n",
    "    print(f\" learing rate : {float(learningRateList[i]):.5f}, accucary : {t}\")\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor learning rate: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Podemos observar en función de su valor de accuracy, que es posible parametrizar `SGDClassifier` de tal manera que obtenga un rendimiento muy similar a `LogisticRegression`, para poder llegar a este valor, se modifica el parámetro **alpha** de `SGDClassifier`, el cual es un parámetro regularizador y además define la tasa de aprendizaje inicial, por lo que al definir valores de alpha más pequeños, es posible obtener mejores valores, pero por contrapartes es más lento a la hora de realizar el fit.\n",
    "\n",
    "Cabe destacar, que en la teoría, esperábamos obtener un tiempo de ejecución mucho menor en el algoritmo de `SGDClassifier` debido a que SGDClassifier es un clasificador lineal generalizado que utilizará Stochastic Gradient Descent y LogisticRegression no, si no que implementa una regresión logística logarítmica regularizada por lo que minimiza la probabilidad logarítmica. Pero este no fue el caso, ambos algoritmos tienen tiempos de ejecución similar, por lo que la decisión de que modelo elegir no es clara, debido a que ambos algoritmos tienen un score muy similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-nearest Neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N : 29, accucary : 0.7394285714285714\n",
      " N : 30, accucary : 0.7405714285714285\n",
      " N : 31, accucary : 0.7411428571428571\n",
      " N : 32, accucary : 0.7405714285714285\n",
      " N : 33, accucary : 0.74\n",
      " N : 34, accucary : 0.7388571428571429\n",
      " N : 35, accucary : 0.7411428571428571\n",
      " N : 36, accucary : 0.7394285714285714\n",
      " N : 37, accucary : 0.744\n",
      " N : 38, accucary : 0.7451428571428571\n",
      " N : 39, accucary : 0.7502857142857143\n",
      " N : 40, accucary : 0.7462857142857143\n",
      " N : 41, accucary : 0.7451428571428571\n",
      " N : 42, accucary : 0.7462857142857143\n",
      " N : 43, accucary : 0.7462857142857143\n",
      " N : 44, accucary : 0.7457142857142857\n",
      "Mejor accuracy usando el mejor vecino:  0.7502857142857143\n",
      "tiempo elegir mejor:3897 segundos\n"
     ]
    }
   ],
   "source": [
    "vecinos = range(29,45)\n",
    "accList = []\n",
    "maxAcc = 0\n",
    "start = time.time() \n",
    "\n",
    "for vecino in vecinos:\n",
    "    model_5 = KNeighborsClassifier(vecino, n_jobs=4)\n",
    "\n",
    "    pipe6 = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', model_5)])\n",
    "    pipe6.fit(X_train, y_train)\n",
    "\n",
    "    predicted_model_5 = pipe6.predict(X_test) \n",
    "    \n",
    "    t = float(metrics.accuracy_score(y_test, predicted_model_5))\n",
    "    accList.append(t)\n",
    "    \n",
    "    if  (t > maxAcc):\n",
    "        maxAcc = metrics.accuracy_score(y_test, predicted_model_5)\n",
    "    print(f\" N : {int(vecino)}, accucary : {t}\")\n",
    "    \n",
    "end = time.time()\n",
    "       \n",
    "print(\"Mejor accuracy usando el mejor vecino: \", maxAcc)\n",
    "end = time.time()\n",
    "print(\"tiempo elegir mejor:\" + str(int(end - start)), \"segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Podemos observar que el accuracy de KNN es bastante cercano al mejor (LogisicRegresion), siendo este de 0.7502, utilizando 39 vecinos.\n",
    "\n",
    "Cabe destacar que este método es bastante más costoso a nivel computacional y la elección de el valor de k, no es muy facil de determinar, debido a que en nuestro caso, la diferencia entre cada valor de k no es tan radical como en los modelos pasados, donde en cada iteración habia un notorio aumento de decrecimiento del valor de accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones generales\n",
    "\n",
    "Podemos apreciar que todos los modelos implementados tienen valores de accuracy bastante similares y a juicio personal bastante buenos, por lo que creo que implementar alguno de estos métodos tradicionales es bastante óptimo para el problema en cuestión.\n",
    "\n",
    "Si se tuviera que seleccionar un método en concreto, seleccionaría `SGDClassifier`, debido a que al no ser el que determinó el mejor valor de accuracy, este tiene un menor costo computacional. Además que los parámetros que tiene, dan la posibilidad de hacer un parameter tuning mucho mejor que LogisticRegression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTENTO DE IMPLEMENTAR BETO DCCUCHILE (NO FUNCIONA)\n",
    "\n",
    "\n",
    "* La intención era realizar una implementación de BETO (Bert entrenado en español https://github.com/dccuchile/beto) para la clasificación de textos, pero no funciona (lo dejo por si acaso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acotar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"\"\"SELECT * FROM df WHERE category = \"tendencias\";\"\"\"\n",
    "df_tend=sqldf(q)\n",
    "\n",
    "df_tend = df_tend.sample(n=100)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"tecnologias\";\"\"\"\n",
    "df_tech = sqldf(q)\n",
    "\n",
    "df_tech = df_tech.sample(n=100)\n",
    "\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"pais\";\"\"\"\n",
    "df_pais=sqldf(q)\n",
    "\n",
    "df_pais = df_pais.sample(n=100)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"mundo\";\"\"\"\n",
    "df_mundo=sqldf(q)\n",
    "\n",
    "df_mundo = df_mundo.sample(n=100)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"economia\";\"\"\"\n",
    "df_eco=sqldf(q)\n",
    "\n",
    "df_eco = df_eco.sample(n=100)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"deportes\";\"\"\"\n",
    "df_dep = sqldf(q)\n",
    "\n",
    "df_dep = df_dep.sample(n=100)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE category = \"cultura\";\"\"\"\n",
    "df_cult = sqldf(q)\n",
    "\n",
    "df_cult = df_cult.sample(n=100)\n",
    "\n",
    "df = pd.concat([df_tend, df_tech, df_pais, df_mundo,df_eco,df_dep,df_cult], ignore_index=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reemplazamos las categorias por valores numeros debido a que tensor no aceptar strings.\n",
    "\n",
    "df['category'] = df['category'].str.replace('cultura','0')\n",
    "df['category'] = df['category'].str.replace('deportes','1')\n",
    "df['category'] = df['category'].str.replace('economia','2')\n",
    "df['category'] = df['category'].str.replace('mundo','3')\n",
    "df['category'] = df['category'].str.replace('pais','4')\n",
    "df['category'] = df['category'].str.replace('tecnologias','5')\n",
    "df['category'] = df['category'].str.replace('tendencias','6')\n",
    "\n",
    "df['category'] = df['category'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Usamos cuda para poder utilizar GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Se realiza un split del dataset\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['category'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidation\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluar el modelo y crear el tokenizador\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
    "bert = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo\n",
    "text = ['Confución generaron los datos entregados por la Oficina Nacional de Emergencia con respecto a la cantidad de casas afectadas por el incendio en la comuna de Parral. La primera información oficial fue entregada este miércoles un cuarto para la 1 de la madrugada, donde se aseguraba había 40 viviendas destruidas. Sin embargo, pasadas las 19:30 del mismo día hicieron la siguiente declaración. Desde el Gobierno se justificaron y afirman que la Onemi no incurrió en error, puesto que siempre se habló de un número “provisorio”. Los detalles en el video adjunto.  ','Una nueva polémica tiene esta carrera presidencial. El sobrino del Alejandro Navarro y actual candidato a Core, Eduardo Hernández Navarro, intentó funar un acto de Sebastián Piñera en Concepción. Según indicó radio Biobío, el hecho ocurrió este lunes en las cercanías del Parque Ecuador en Concepción, donde el abanderado de Chile Vamos realizaba un encuentro con gente de la zona. Posteriormente, miembros del equipo de seguridad de Piñera retiraron a Hernández del lugar y luego Carabineros lo mantuvo retenido. Revisa parte de esta acción en el video adjunto.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, \n",
    "                                      return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[4, 9741, 1587, 12359, 1022, 1065, 2521, 29221, 1096, 1030, 3884, 3283, 1008, 28717, 1051, 2440, 1013, 1030, 3990, 1008, 6976, 10986, 1096, 1040, 11707, 1036, 1030, 11997, 1008, 1561, 2411, 1009, 1198, 2076, 1924, 3844, 1341, 7217, 1093, 1365, 13107, 1049, 4181, 1110, 1030, 1094, 1008, 1030, 22094, 1017, 1748, 1062, 15744, 1213, 1824, 3242, 8708, 21142, 30934, 1009, 2169, 2415, 1017, 24268, 1089, 1242, 1181, 2101, 1072, 1698, 1726, 5353, 1030, 2435, 4428, 1009, 3217, 1040, 2684, 1062, 14596, 1022, 1042, 21606, 1038, 1030, 17087, 1155, 1084, 11920, 2852, 1036, 5029, 1017, 3134, 1038, 2032, 1062, 10470, 1008, 1049, 2272, 3, 5280, 15459, 3, 1009, 1412, 7113, 1036, 1040, 6128, 16607, 1009, 5], [4, 1965, 2744, 24783, 1512, 1359, 4190, 15122, 1009, 1162, 15473, 1072, 12630, 26955, 1042, 3627, 9738, 1013, 2167, 30931, 1017, 12593, 18253, 26955, 1017, 11274, 1545, 1018, 1049, 5983, 1008, 16174, 5552, 13961, 1048, 1036, 18834, 1009, 3678, 14351, 4707, 12923, 7920, 30933, 1017, 1040, 1802, 7176, 1365, 8850, 1036, 1089, 22865, 1072, 10737, 9791, 1036, 18834, 1017, 1748, 1040, 3940, 1188, 1047, 1008, 5571, 1716, 29412, 1049, 6160, 1051, 2114, 1008, 1030, 2785, 1009, 9137, 1017, 1923, 1072, 2520, 1008, 2264, 1008, 5552, 13961, 1048, 14971, 1022, 1013, 18253, 1072, 1675, 1042, 2475, 6325, 16974, 1754, 1114, 9921, 15718, 1459, 1009, 13702, 30932, 1565, 1008, 1359, 3463, 1036, 1040, 6128, 16607, 1009, 5]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "## Para ver como funciona batch_enconde_plus\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficamos para determinar el largo maximo de las frases, se detemrina la mayoria tienen un\n",
    "# largo máximo de 200.\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/miniconda3/envs/IA/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      (_, cls_hs) = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/miniconda3/envs/IA/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5 6], y=545    1\n",
      "320    3\n",
      "39     6\n",
      "344    3\n",
      "396    3\n",
      "      ..\n",
      "292    4\n",
      "147    5\n",
      "161    5\n",
      "300    3\n",
      "455    2\n",
      "Name: category, Length: 490, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model pred ictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c5138ddf6b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-1ac28d74aea2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# get model predictions for the current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# compute the loss between actual and predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-c4bf463c2a8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m#pass the inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Pese a que no funciona, la intención era comprabar que tan optimos son los metodos tradicionales en comparación a BERT, modelo sumamente popular debido a sus buenos resultados, el analisis no pudo ser concretado debido a un error que no pude resolver, pero se seguirá con la intención de poder solucionarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
